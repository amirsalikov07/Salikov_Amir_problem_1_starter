# Salikov_Amir_problem_1_starter
LBL1 - выбираем устройство вычислений <br>
LBL2 - фиксируем число классов для нашей задачи и статистики нормализации как в ImageNet <br>
LBL3  - загрузка предобученной модели EfficientNet-B0 <br>
LBL4  - замена «головы» под число классов <br> 
LBL5  - заморозка всего, кромел последнего слоя <br>
LBL6  - функция потерь и оптимизатор <br>
LBL7 -  трансформации данных (train/eval) и путь для сохранения <br>
LBL8 - преобразование картинки в тензор <br>
LBL9 - сохраняем веса модели<br>
LBL10 - если локального .pth нет — пробуем скачать<br>
LBL11 - загружаем чекпойнт PyTorch<br>
LBL12 - восстанавливаем метаданные<br>
LBL13 - подгружаем веса<br>
LBL14 -  обновляем параметры препроцессинга и пересобираем трансформации<br>
LBL15 - переводим модель в режим инференса, отключаем градиенты, проходим по даталоадеру, аккумулируя суммарный loss и число верных предсказаний, а в конце возвращаем средний loss и accuracy в процентах.<br>
LBL16 - преобразуем обучающую и тестовую выборки в DataLoader<br>
LBL17 - фиксируем число эпох (30)<br>
LBL18 - инициализируем списки значений функции потерь и точности на обучающей и тестовой выборке, для дальнейшего отображения их на графике<br>
LBL19 - начинаем цикл обучения<br>
LBL20 - обнуляем градиенты<br>
LBL21 - делаем прямой проход<br>
LBL22 - вычисляем функцию потерь<br>
LBL23 - считаем градиенты и изменяем веса<br>
LBL24 - считаем потери и точность на обучающей выборке<br>
LBL25 - считаем потери и точность на тестовой выборке<br>
LBL26 - выводим потери и точность во время обучения на каждой эпохе<br>
LBL27 - прямой проход модели на тестовом изображении<br>
LBL28 - отображаем график точности на обучающей и тестовой выборках<br>
LBL29 - отображаем график потерь на обучающей и тестовой выборках<br>
